{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "from google.cloud import vision_v1 as vision\n",
    "from google.cloud.vision_v1 import types\n",
    "from google.cloud import translate_v2 as translate\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from enum import Enum\n",
    "from termcolor import colored\n",
    "from itertools import groupby\n",
    "import statistics\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'Demz_vision_API_token.json' # google API credentials\n",
    "\n",
    "# Path to the rc1\n",
    "rc1_1 = 'rc1-1.jpg'\n",
    "rc1_2 = 'rc1-2.jpg'\n",
    "FOLDER_PATH = 'C:\\\\Users\\\\USER\\\\Desktop\\\\NEW_DEMZ\\\\OCR\\OCR_Works\\\\Google\\\\rc1\\\\images'\n",
    "\n",
    "image = rc1_1\n",
    "\n",
    "# Google API client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# Open and Read contents on the documents\n",
    "with io.open(os.path.join(FOLDER_PATH, image), 'rb') as image_file1:\n",
    "        content = image_file1.read()\n",
    "content_image = types.Image(content=content) # Reading the Image Content\n",
    "rc1_response = client.text_detection(image=content_image) # Text Detection \n",
    "rc1_texts = rc1_response.text_annotations # Text Response\n",
    "\n",
    "# face detection\n",
    "response_face = client.face_detection(image=content_image)\n",
    "faceAnnotations = response_face.face_annotations\n",
    "\n",
    "\n",
    "def draw_boxes(image, bounds,faceAnnotations, color,width=2):\n",
    "    \"\"\"This functionality draws bounding box on the detected texts and around the faces if any\"\"\"\n",
    "    image = Image.open(image)\n",
    "#     image = resize_Image(img)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for text in bounds:\n",
    "        draw.line([\n",
    "            text.bounding_poly.vertices[0].x, text.bounding_poly.vertices[0].y, # left\n",
    "            text.bounding_poly.vertices[1].x, text.bounding_poly.vertices[1].y, # top\n",
    "            text.bounding_poly.vertices[2].x, text.bounding_poly.vertices[2].y, # right\n",
    "            text.bounding_poly.vertices[3].x, text.bounding_poly.vertices[3].y, # bootom\n",
    "            text.bounding_poly.vertices[0].x, text.bounding_poly.vertices[0].y],fill=color, width=width)\n",
    "        txt=text.description\n",
    "        size = 13\n",
    "        font=ImageFont.truetype(\"fonts/arial.ttf\",size)\n",
    "        \n",
    "        draw.text((text.bounding_poly.vertices[0].x, text.bounding_poly.vertices[3].y), txt, font=font, fill=\"#000\")\n",
    "    try: # check if face or passport is in the document\n",
    "        for face in faceAnnotations:\n",
    "            draw.line([\n",
    "                face.bounding_poly.vertices[0].x, face.bounding_poly.vertices[0].y,\n",
    "                face.bounding_poly.vertices[1].x, face.bounding_poly.vertices[1].y,\n",
    "                face.bounding_poly.vertices[2].x, face.bounding_poly.vertices[2].y,\n",
    "                face.bounding_poly.vertices[3].x, face.bounding_poly.vertices[3].y,\n",
    "                face.bounding_poly.vertices[0].x, face.bounding_poly.vertices[0].y],fill='yellow', width=width)\n",
    "            txt='Face'\n",
    "            size = 13\n",
    "            font=ImageFont.truetype(\"fonts/arial.ttf\",size)\n",
    "            draw.text((face.bounding_poly.vertices[0].x, face.bounding_poly.vertices[3].y), txt, font=font, fill=\"#000\")\n",
    "    except:\n",
    "        pass\n",
    "    return image\n",
    "\n",
    "draw_boxes(os.path.join(FOLDER_PATH, image),rc1_texts, faceAnnotations, 'blue')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Image Detected\n"
     ]
    }
   ],
   "source": [
    "# Get crop hint\n",
    "crop_hints_params = vision.CropHintsParams(aspect_ratios=[1.77])\n",
    "def get_crop_hint(crop_hints):\n",
    "    \"\"\"Detect crop hints on a single image and return the first result.\"\"\"\n",
    "    with io.open(crop_hints, 'rb') as image_file1:\n",
    "        content = image_file1.read()\n",
    "\n",
    "    # content = crop_hints\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    image_context = vision.ImageContext(crop_hints_params=crop_hints_params)\n",
    "\n",
    "    response = client.crop_hints(image=image, image_context=image_context)\n",
    "    hints = response.crop_hints_annotation.crop_hints\n",
    "\n",
    "    # Get bounds for the first crop hint using an aspect ratio of 1.77.\n",
    "    vertices = hints[0].bounding_poly.vertices\n",
    "\n",
    "    return vertices\n",
    "\n",
    "def crop_Face_hint(image_file, faceAnnotations):\n",
    "    \"\"\"Crop the image using the hints in the vector list.\"\"\"\n",
    "    vects = get_crop_hint(image_file)\n",
    "    faceAnnotations = faceAnnotations\n",
    "    try:\n",
    "        face_bounds = []\n",
    "        for face in faceAnnotations:\n",
    "            face_vertices = (['({0},{1})'.format(vertex.x, vertex.y) for vertex in face.bounding_poly.vertices])\n",
    "            face_bounds.append(face_vertices)\n",
    "        bound_1 = face_bounds[0][0]\n",
    "        bound_2 = face_bounds[0][2]\n",
    "        bound_1 = eval(bound_1)\n",
    "        bound_2 = eval(bound_2)\n",
    "\n",
    "        im = Image.open(image_file)\n",
    "\n",
    "        im2 = im.crop([bound_1[0], bound_1[1],\n",
    "                      bound_2[0] - 1, bound_2[1] - 1])\n",
    "        return im2\n",
    "    except:\n",
    "        print(\"No Image Detected\")\n",
    "    \n",
    "face = crop_Driver_hint(os.path.join(FOLDER_PATH, image), faceAnnotations)\n",
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_the_texts(rc1_response):\n",
    "\n",
    "    # rearrange the Response using the bbox values\n",
    "    items = []\n",
    "    lines = {}\n",
    "\n",
    "    for text in rc1_response.text_annotations[1:]:\n",
    "        left_x_axis = text.bounding_poly.vertices[0].x # top left\n",
    "        left_y_axis = text.bounding_poly.vertices[0].y # top left\n",
    "        top_x_axis = text.bounding_poly.vertices[1].x # top\n",
    "        top_y_axis = text.bounding_poly.vertices[1].y # top\n",
    "        right_x_axis = text.bounding_poly.vertices[2].x # right\n",
    "        right_y_axis = text.bounding_poly.vertices[2].y # right\n",
    "        bottom_x_axis = text.bounding_poly.vertices[3].x # bottom\n",
    "        bottom_y_axis = text.bounding_poly.vertices[3].y # bottom\n",
    "\n",
    "        if left_y_axis not in lines:\n",
    "            lines[left_y_axis] = [(left_y_axis, bottom_y_axis), []]\n",
    "        for s_top_y_axis, s_item in lines.items():\n",
    "            if left_y_axis < s_item[0][1]:\n",
    "                lines[s_top_y_axis][1].append(([left_x_axis, bottom_y_axis, top_x_axis], text.description))\n",
    "                break\n",
    "    for _, item in lines.items():\n",
    "        if item[1]:\n",
    "            words = sorted(item[1], key=lambda t: t[0])\n",
    "            items.append((item[0], ' '.join([word for _, word in words]), words))\n",
    "\n",
    "    # print(items)\n",
    "\n",
    "    # Find the most common space\n",
    "    def find_common_space(items):\n",
    "        spaces = []\n",
    "        for i,k in enumerate(items):\n",
    "            try:\n",
    "                calculated_space = items[i][2][0][0][1] - items[i -1][2][0][0][1]\n",
    "                spaces.append(calculated_space)\n",
    "            except:\n",
    "                ...\n",
    "\n",
    "        common_space = abs(statistics.median(spaces))\n",
    "\n",
    "        return common_space\n",
    "\n",
    "    print(find_common_space(items))\n",
    "    # print(len(items))\n",
    "\n",
    "    # Split the content based on horizontal space\n",
    "    new_content = []\n",
    "\n",
    "    most_common_space = find_common_space(items)\n",
    "    print(most_common_space)\n",
    "\n",
    "    len_content = len(items) / 5.5\n",
    "\n",
    "    for i,k in enumerate(items):\n",
    "        try:\n",
    "            if abs(items[i][2][0][0][1] - items[i -1][2][0][0][1]) >= most_common_space -len_content:\n",
    "                new_content.append([])\n",
    "                new_content.append(items[i][2])\n",
    "            else:\n",
    "                new_content.append(items[i][2])\n",
    "        except:\n",
    "            new_content.append(items[i][2])\n",
    "\n",
    "\n",
    "    # group the contents based on horizontal space\n",
    "    new_content = [list(l) for i, l in groupby(new_content, bool) if i]\n",
    "\n",
    "    # join list that the length is greater than one\n",
    "    for i in range(len(new_content)):\n",
    "        if len(new_content[i]) > 1:\n",
    "            new_content[i] = sum(new_content[i], [])\n",
    "            new_content[i] = [new_content[i]]\n",
    "        else:\n",
    "            new_content[i] = new_content[i]\n",
    "\n",
    "    # find common width of white space on the document\n",
    "    def find_common_diff(content):\n",
    "        diffs = []\n",
    "        for i,j in enumerate(new_content):\n",
    "            for u,t in enumerate(new_content[i][0]):\n",
    "                try:\n",
    "                    difference = abs(new_content[i][0][u][0][2] - new_content[i][0][u-1][0][0])\n",
    "                    diffs.append(difference)\n",
    "                except:\n",
    "                    ...\n",
    "        common_diffs = abs(statistics.median(diffs))\n",
    "        return common_diffs\n",
    "\n",
    "    # print(find_common_diff(new_content))\n",
    "\n",
    "    # find common width of white space on a line\n",
    "    def find_common_diff_in_line(content):\n",
    "        diffs = []\n",
    "        for i,j in enumerate(new_content):\n",
    "            temp_diffs = []\n",
    "            for u,t in enumerate(new_content[i][0]):\n",
    "                try:\n",
    "                    difference = abs(new_content[i][0][u][0][0] - new_content[i][0][u-1][0][2])\n",
    "                    temp_diffs.append(difference)\n",
    "                except:\n",
    "                    ...\n",
    "            diffs.append(temp_diffs)\n",
    "        return diffs\n",
    "\n",
    "    # print(find_common_diff_in_line(new_content))\n",
    "\n",
    "    def check_for_max_width(new_content):\n",
    "        widths = []\n",
    "        for i,j in enumerate(new_content):\n",
    "            temp_diffs = []\n",
    "            for u,t in enumerate(new_content[i][0]):\n",
    "                try:\n",
    "                    text_width = abs(new_content[i][0][u][0][0] - new_content[i][0][u][0][2])\n",
    "                    temp_diffs.append(text_width)\n",
    "                except:\n",
    "                    ...\n",
    "            widths.append(temp_diffs)\n",
    "        max_width = max(widths)\n",
    "        return widths\n",
    "    # print(check_for_max_width(new_content))\n",
    "\n",
    "\n",
    "    # split line content based on vertical spacing\n",
    "    contents = []\n",
    "    max_text_width = 600\n",
    "    most_common_diffs = find_common_diff(new_content)\n",
    "\n",
    "    for i,j in enumerate(new_content):\n",
    "        temp_content = []\n",
    "        most_common_diffs_in_a_line = abs(statistics.median(find_common_diff_in_line(new_content)[i]))\n",
    "        text_width = check_for_max_width(new_content)[i]\n",
    "        max_width = max(text_width)\n",
    "    #     print('')\n",
    "        for u,t in enumerate(new_content[i][0]):\n",
    "            try:\n",
    "                difference = abs(new_content[i][0][u][0][0] - new_content[i][0][u-1][0][2])\n",
    "                if most_common_diffs >= most_common_diffs_in_a_line and max_width < max_text_width:\n",
    "                    if difference >= most_common_diffs_in_a_line+20: # getting the min width of white space in that line\n",
    "                        temp_content.append([])\n",
    "                        temp_content.append(new_content[i][0][u])\n",
    "\n",
    "                    else:\n",
    "                        temp_content.append(new_content[i][0][u]) \n",
    "                elif most_common_diffs >= most_common_diffs_in_a_line and max_width > max_text_width:\n",
    "                    if difference > most_common_diffs_in_a_line-15: # getting the min width of white space in that line\n",
    "                        temp_content.append([])\n",
    "                        temp_content.append(new_content[i][0][u])\n",
    "\n",
    "                    else:\n",
    "                        temp_content.append(new_content[i][0][u])\n",
    "                else:\n",
    "                    if difference > most_common_diffs-65: # getting the most common width of white space in the doc\n",
    "                        temp_content.append([])\n",
    "                        temp_content.append(new_content[i][0][u])\n",
    "\n",
    "                    else:\n",
    "                        temp_content.append(new_content[i][0][u])\n",
    "            except:\n",
    "                ...\n",
    "    #     print(temp_content)\n",
    "        new_temp_content = [list(l) for i, l in groupby(temp_content, bool) if i]\n",
    "        contents.append(new_temp_content)    \n",
    "    #     contents.append(temp_content)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keys_and_values(contents):\n",
    "    \"\"\" This functionality extracts they keys and values as in the document structure\"\"\"\n",
    "    def check_for_non_english(new_content):\n",
    "        \"\"\"this function Counts the number of non-english text in a line\"\"\"\n",
    "        max_pos = 800 # this is the max starting position that an english word does exceed\n",
    "        translate_client = translate.Client()\n",
    "        target = 'en'\n",
    "        detected_words = []\n",
    "        for i,j in enumerate(new_content):\n",
    "            temp_words = []\n",
    "            for u,t in enumerate(new_content[i]):\n",
    "                counts = {'en':0, 'af':0}\n",
    "                left_bbox = contents[i][u][0][0][0]\n",
    "                for s,r in enumerate(new_content[i][u]):\n",
    "                    try:\n",
    "                        text = new_content[i][u][s][1]\n",
    "                        output = translate_client.translate(text, target_language=target)\n",
    "                        if output['detectedSourceLanguage'] == 'en' or left_bbox < max_pos:\n",
    "                            counts['en'] += 1\n",
    "                        else:\n",
    "                            counts['af'] += 1\n",
    "                    except:\n",
    "                        ...\n",
    "                temp_words.append(counts)\n",
    "            detected_words.append(temp_words)\n",
    "        return detected_words\n",
    "\n",
    "    word_detection = check_for_non_english(contents)\n",
    "\n",
    "    new_contents = []\n",
    "    max_position = 1200\n",
    "    list_of_special_keywords = ['RC1', 'RNC', 'RTS', 'NRW'] # special key words to add to the extracted values\n",
    "\n",
    "    for i,j in enumerate(contents): # \n",
    "        temp_content = []\n",
    "        word_detected = word_detection[i]\n",
    "        content = contents[i]\n",
    "        for u,t in enumerate(contents[i]):\n",
    "            left_bbox = contents[i][u][0][0][0]\n",
    "            if word_detected[u]['en'] > word_detected[u]['af'] and left_bbox < max_position or contents[i][u][0][1] in list_of_special_keywords:\n",
    "                temp_content.append(content[u])\n",
    "        new_contents.append(temp_content)\n",
    "\n",
    "    # get the left bbox of the keys\n",
    "    left_bbox_lists = []\n",
    "    for i in range(len(new_contents)):\n",
    "        try:\n",
    "            left_bbox = new_contents[i][0][0][0][0]\n",
    "            left_bbox_lists.append(left_bbox)\n",
    "        except:\n",
    "            left_bbox = 0\n",
    "            left_bbox_lists.append(left_bbox)\n",
    "    print(left_bbox_lists)\n",
    "    max_left_bbox = max(left_bbox_lists)\n",
    "\n",
    "\n",
    "    # getting the keys and values \n",
    "    rc1_contents = []\n",
    "    max_starting_pos = 250\n",
    "\n",
    "    for i in range(len(new_contents)):\n",
    "        keys = ''\n",
    "        values = ''\n",
    "        for u in range(len(new_contents[i])):\n",
    "            left_bbox = new_contents[i][u][0][0][0]\n",
    "            if left_bbox <= max_left_bbox and left_bbox <=max_starting_pos:\n",
    "                try:\n",
    "                    for j in range(len(new_contents[i][u])):\n",
    "                        text = new_contents[i][u][j][1]\n",
    "                        keys += ' '+text\n",
    "                except:\n",
    "                    ...\n",
    "            else:\n",
    "                try:\n",
    "                    for j in range(len(new_contents[i][u])):\n",
    "                        text = new_contents[i][u][j][1]\n",
    "                        values += ' '+text\n",
    "                except:\n",
    "                    ...\n",
    "        # if the content of the current line does not have a key but has a value. \n",
    "        # if so join the value with the value of the previous item in the rc1_contents.\n",
    "        try:\n",
    "            last_key = [i for i,j in rc1_contents[-1].items()][0] # previous item key\n",
    "            last_value = [j for i,j in rc1_contents[-1].items()][0] # previous item value\n",
    "        except:\n",
    "            ...\n",
    "        if keys == '' and values != '' and last_key and last_value:   # check for condition\n",
    "            k = '' \n",
    "            val = ''\n",
    "            for key, value in rc1_contents[-1].items():\n",
    "                if key and value:\n",
    "                    val = value \n",
    "                    val += ' '+values\n",
    "                    k = key\n",
    "    #                 print(key, value)\n",
    "                else:\n",
    "                    k = key\n",
    "                    val = value\n",
    "            rc1_contents.remove(rc1_contents[-1])\n",
    "            rc1_contents.append({k:val})\n",
    "        else:\n",
    "            rc1_contents.append({keys:values})\n",
    "    return rc1_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.0\n",
      "42.0\n",
      "[105, 154, 210, 114, 112, 113, 114, 113, 114, 112, 114, 113, 113, 114, 113, 114, 114, 113, 113, 114, 114, 115, 114, 113, 115, 114, 115, 114, 115, 115, 114, 114, 114, 114, 114, 115, 116, 116, 116, 116, 0, 0, 187]\n"
     ]
    }
   ],
   "source": [
    "contents = arrange_the_texts(rc1_response)\n",
    "contents = extract_keys_and_values(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' RET(7)(2005/02)': ' RC1'}\n",
      "{' Republic of South Africa': ''}\n",
      "{' CERTIFICATE OF REGISTRATION IN RESPECT OF MOTOR VEHICLE (National Road Traffic Act, 1996)': ' IKE E'}\n",
      "{' Registering authority': ' Johannesburg'}\n",
      "{' Vehicle register number': ' ZJX226W'}\n",
      "{' Vehicle identification number (VIN)': ' MRHRU183OJP040138'}\n",
      "{' Engine number': ' L15Z73500189'}\n",
      "{' Make': ' HONDA'}\n",
      "{' Series name': ' HR-V'}\n",
      "{' Vehicle category': ' Light passenger mv (less than 12 persons)'}\n",
      "{' Driven': ' Self-propelled / Selfgedrewe'}\n",
      "{' Vehicle description': ' Hatch back / Luikrug'}\n",
      "{' Tare (T): kg': ' 1169'}\n",
      "{' Date of liability for first licensing (Not year model)': ' 2019-05-10'}\n",
      "{' Vehicle status': ' New / Nuut'}\n",
      "{' Date liable for registration': ' 2019-05-10'}\n",
      "{' Last 3 licence numbers (most recent first, if available)': ''}\n",
      "{' TITLE HOLDER': ''}\n",
      "{' Type of identification': ' Reg no certificate / Reg nr sertifikaat'}\n",
      "{' Identification number': ' 40242J9LY004'}\n",
      "{' Country of issue': ' South Africa / Suid-Afrika'}\n",
      "{' Name': ' NKONGE DG'}\n",
      "{' OWNER': ''}\n",
      "{' Type of identification': ' Reg no certificate / Reg nr sertifikaat'}\n",
      "{' Identification number': ' 40242J9LY0004'}\n",
      "{' Country of issue': ' South Africa / Suid-Afrika'}\n",
      "{' Name': ' NKONGE DG'}\n",
      "{' Control number': ' 4024047M36WW'}\n",
      "{' Issue number': ' 01'}\n",
      "{' Date of issue': ' 2019-05-10'}\n",
      "{' at Registering which registered authority': ' Johannesburg'}\n",
      "{' RECEIPT': ''}\n",
      "{' Receipt number': ' 4024053HMBND'}\n",
      "{' Transaction': ' Vehicle registration/Voertuigregistrasie'}\n",
      "{' Total amount received': ' R180.00'}\n",
      "{' Date': ' 2019-05-10'}\n",
      "{' Received by': ' MW MALLANE'}\n",
      "{' Method of payment': ' Multiple adding / Veelvoudige optelling'}\n",
      "{' Number': ''}\n",
      "{' 4024': ' 2019-05-10 15:28:30'}\n",
      "{'': ''}\n",
      "{'': ''}\n",
      "{' ISSUED WITHOUT ANY ALTERATIONS OR ERASURES': ''}\n"
     ]
    }
   ],
   "source": [
    "for i in contents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
